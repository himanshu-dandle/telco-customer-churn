# -*- coding: utf-8 -*-
"""customer_churn_prediction.ipynb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11u6GAvrTkaq7OFfcEEEp_RNydPn1rSyb

##  Import Libraries and Load Data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve

# Load dataset
data_path = '../data/telco_customer_churn.csv'
data = pd.read_csv(data_path)
data.head()

#  Data Exploration
print("Basic Information:")
print(data.info())
print("\nSummary Statistics:")
print(data.describe())

##Check for Missing Values
missing_values = data.isnull().sum()
print("Missing Values per Column:")
print(missing_values)

# 4. Handle Missing Values
data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')
data['TotalCharges'].fillna(data['TotalCharges'].median(), inplace=True)

# Ensure the columns for correlation are numeric and handle any non-numeric values
# Convert 'TotalCharges' to numeric, with errors='coerce' to handle non-numeric values
data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')

# Check if there are any missing values after conversion
missing_values = data[['tenure', 'MonthlyCharges', 'TotalCharges']].isnull().sum()
print("Missing values after conversion:")
print(missing_values)

# Fill missing values with median in the 'TotalCharges' column (if any)
data['TotalCharges'].fillna(data['TotalCharges'].median(), inplace=True)

## Univariate Analysis (Distribution of Key Features)

plt.figure(figsize=(15, 8))

plt.subplot(2, 2, 1)
sns.histplot(data['tenure'], kde=True, bins=20)
plt.title('Distribution of Tenure')

plt.subplot(2, 2, 2)
sns.histplot(data['MonthlyCharges'], kde=True, bins=20)
plt.title('Distribution of Monthly Charges')

plt.subplot(2, 2, 3)
sns.histplot(data['TotalCharges'], kde=True, bins=20)
plt.title('Distribution of Total Charges')

plt.subplot(2, 2, 4)
sns.countplot(x='Churn', data=data)
plt.title('Churn Distribution')
plt.tight_layout()
plt.show()

"""## Bivariate Analysis (Relationship between Features)"""

# Tenure vs MonthlyCharges, colored by Churn
plt.figure(figsize=(6, 6))
sns.scatterplot(x='tenure', y='MonthlyCharges', hue='Churn', data=data)
plt.title('Tenure vs Monthly Charges by Churn')
plt.show()

# MonthlyCharges vs TotalCharges, colored by Churn
plt.figure(figsize=(6, 6))
sns.scatterplot(x='MonthlyCharges', y='TotalCharges', hue='Churn', data=data)
plt.title('Monthly Charges vs Total Charges by Churn')
plt.show()

# Correlation Matrix (Numerical Features Only)
plt.figure(figsize=(10, 8))
correlation_matrix = data[['tenure', 'MonthlyCharges', 'TotalCharges']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='Blues')
plt.title('Correlation Matrix')
plt.show()

# Encode Categorical Features
data = pd.get_dummies(data, drop_first=True)

# Split the Dataset into Features and Target Variable
X = data.drop('Churn_Yes', axis=1)
y = data['Churn_Yes']



"""## Handle Outliers"""

# Function to identify outliers using IQR
def detect_outliers_iqr(data, feature):
    Q1 = data[feature].quantile(0.25)
    Q3 = data[feature].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return data[(data[feature] < lower_bound) | (data[feature] > upper_bound)]

# Identify outliers for MonthlyCharges
outliers_monthly_charges = detect_outliers_iqr(data, 'MonthlyCharges')
print(f'Outliers in MonthlyCharges: {len(outliers_monthly_charges)}')

# Identify outliers for Tenure
outliers_tenure = detect_outliers_iqr(data, 'tenure')
print(f'Outliers in Tenure: {len(outliers_tenure)}')

import seaborn as sns
import matplotlib.pyplot as plt

# Plot the distribution of numeric features
plt.figure(figsize=(15, 5))

# Tenure distribution
plt.subplot(1, 3, 1)
sns.boxplot(data['tenure'])
plt.title('Tenure')

# MonthlyCharges distribution
plt.subplot(1, 3, 2)
sns.boxplot(data['MonthlyCharges'])
plt.title('MonthlyCharges')

# TotalCharges distribution
plt.subplot(1, 3, 3)
sns.boxplot(data['TotalCharges'])
plt.title('TotalCharges')

plt.tight_layout()
plt.show()



"""## Data Preprocessing"""

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 12. Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""## Model Building and Training"""

# Train a Random Forest Classifier
model = RandomForestClassifier(random_state=42)
model.fit(X_train_scaled, y_train)

# Predictions on test set
y_pred = model.predict(X_test_scaled)

# Performance evaluation
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# ROC-AUC score
roc_auc = roc_auc_score(y_test, model.predict_proba(X_test_scaled)[:, 1])
print(f'ROC-AUC Score: {roc_auc}')

# Plot confusion matrix
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

## Plot AUC-ROC Curve
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve

y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})', color='blue')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

"""## try other classification algothrims"""

from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve

import os

# Create output directory if it doesn't exist
output_path = '../output'
if not os.path.exists(output_path):
    os.makedirs(output_path)

# Define a function to train models, get ROC-AUC curves, and save plots
def plot_roc_curve_and_save(model, X_test, y_test, model_name, output_file):
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')
    plt.legend()

    # Save the plot
    plt.savefig(os.path.join(output_path, output_file))

"""##  Train and Evaluate Other  Models"""

##Random Forest
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train_scaled, y_train)

# 10.2 Logistic Regression
lr_model = LogisticRegression(random_state=42)
lr_model.fit(X_train_scaled, y_train)

# 10.3 Support Vector Machine (SVM) with probability enabled
##svm_model = SVC(probability=True, random_state=42)
##svm_model.fit(X_train_scaled, y_train)

# SVM with linear kernel for faster performance
svm_model = SVC(kernel='linear', probability=True, random_state=42)
svm_model.fit(X_train_scaled, y_train)

# 10.4 Decision Tree
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train_scaled, y_train)

#  Naive Bayes
nb_model = GaussianNB()
nb_model.fit(X_train_scaled, y_train)

# XGBoost Classifier
xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
xgb_model.fit(X_train_scaled, y_train)

# 8.7 Gradient Boosting Classifier
gb_model = GradientBoostingClassifier(random_state=42)
gb_model.fit(X_train_scaled, y_train)



# 9. Plot ROC-AUC Curves for All Models and Save Plots
plt.figure(figsize=(10, 8))

# Random Forest
plot_roc_curve_and_save(rf_model, X_test_scaled, y_test, "Random Forest", "roc_random_forest.png")

# Logistic Regression
plot_roc_curve_and_save(lr_model, X_test_scaled, y_test, "Logistic Regression", "roc_logistic_regression.png")

# SVM
##plot_roc_curve_and_save(svm_model, X_test_scaled, y_test, "SVM (Linear Kernel)", "roc_svm.png")

# Decision Tree
plot_roc_curve_and_save(dt_model, X_test_scaled, y_test, "Decision Tree", "roc_decision_tree.png")

# Naive Bayes
plot_roc_curve_and_save(nb_model, X_test_scaled, y_test, "Naive Bayes", "roc_naive_bayes.png")

# XGBoost
plot_roc_curve_and_save(xgb_model, X_test_scaled, y_test, "XGBoost", "roc_xgboost.png")

# Gradient Boosting
plot_roc_curve_and_save(gb_model, X_test_scaled, y_test, "Gradient Boosting", "roc_gradient_boosting.png")

# Plot random guess (diagonal line) and save the overall ROC plot
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for Various Classifiers')
plt.legend()
plt.savefig(os.path.join(output_path, 'roc_all_models.png'))  # Save the combined ROC plot
plt.show()

"""Based on the ROC curves and the AUC scores you provided, hereâ€™s a comparison of the classifiers:

Gradient Boosting: AUC = 0.86 (Best performing model)
Random Forest: AUC = 0.85
Logistic Regression: AUC = 0.85
XGBoost: AUC = 0.83
Decision Tree: AUC = 0.69
Naive Bayes: AUC = 0.50
Conclusion:
The Gradient Boosting classifier is the best performing model with an AUC score of 0.86, closely followed by Random Forest and Logistic Regression with AUC scores of 0.85.
XGBoost is also a strong performer with an AUC of 0.83.
Decision Tree and Naive Bayes have lower AUC scores, making them less effective compared to the other models.

## Explanation for Choosing Random Forest:

Choosing  Random Forest for this project because:

Performance: Random Forest achieved an AUC score of 0.85, demonstrating strong predictive power.
Robustness: It is less prone to overfitting due to its bagging technique, where multiple decision trees are trained on different subsets of the data. This leads to more generalized predictions.
Ease of Use: Random Forest does not require extensive hyperparameter tuning to perform well, making it an efficient choice for many real-world problems.
Interpretability: Random Forest allows us to easily extract feature importance, which can help understand the contribution of each feature to the predictions.
Given these advantages, Random Forest offers a good balance between performance, interpretability, and ease of implementation for this customer churn prediction project.
"""

# Predict using the trained Random Forest model
y_pred_rf = rf_model.predict(X_test_scaled)

# Plot and save the confusion matrix for Random Forest
plt.figure(figsize=(6, 4))
sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap='Blues')
plt.title('Random Forest Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.savefig(os.path.join(output_path, 'random_forest_confusion_matrix.png'))  # Save the plot
plt.show()

"""## Feature Importance in Random Forest"""

# Train the Random Forest model (if not already done)
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train_scaled, y_train)

# Get feature importance from the trained model
importances = rf_model.feature_importances_
feature_names = X.columns

# Create a DataFrame for better readability
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
})

# Sort features by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Display the top features
print("Top 10 Important Features in Random Forest:")
print(feature_importance_df.head(10))

# Plot feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(10))
plt.title('Top 10 Important Features in Random Forest')
# Save the plot before showing it
plt.savefig(os.path.join(output_path, 'random_forest_feature_importance.png'))
plt.show()

"""## Final Analysis

MonthlyCharges (0.32): This feature has the highest importance, meaning that the amount customers are charged monthly has the biggest influence on whether they churn. It suggests that customers who pay more monthly are more likely to churn (or vice versa).

tenure (0.21): The duration for which the customer has stayed with the company is also a significant factor. Longer tenure might indicate loyalty, while shorter tenure could indicate churn risk.

TotalCharges (0.18): The total amount spent by the customer is also crucial, although slightly less important than MonthlyCharges and tenure.

Contract_Two year (0.10): Customers with a two-year contract are less likely to churn, indicating that contract length plays a role in customer retention.

Lower Importance Features: Features like Dependents_Yes (0.01) and TechSupport_Yes (0.01) have very low importance, suggesting they donâ€™t contribute much to the modelâ€™s prediction of churn.

Using Feature Importance in Model Improvement:
Remove Irrelevant Features: If a feature has little to no importance (e.g., Dependents_Yes), you might consider removing it from the dataset to reduce complexity.
Focus on Important Features: Features like MonthlyCharges or tenure might be areas for business improvement (e.g., offering discounts or loyalty benefits to customers with high monthly charges).

## Check for Overfitting in Random Forest
"""

# 1. Compute Accuracy or AUC on Training Set
y_train_pred_rf = rf_model.predict(X_train_scaled)
train_accuracy = rf_model.score(X_train_scaled, y_train)
print(f"Training Accuracy for Random Forest: {train_accuracy:.4f}")

# 2. Compute Accuracy or AUC on Test Set
y_test_pred_rf = rf_model.predict(X_test_scaled)
test_accuracy = rf_model.score(X_test_scaled, y_test)
print(f"Test Accuracy for Random Forest: {test_accuracy:.4f}")

# 3. Compare Training and Test Accuracy
if train_accuracy > test_accuracy:
    print(f"The model might be overfitting (Train Accuracy: {train_accuracy:.4f} vs Test Accuracy: {test_accuracy:.4f})")
else:
    print("The model shows no significant signs of overfitting.")

"""## Cross-Validation to Detect Overfitting"""

from sklearn.model_selection import cross_val_score

# Perform 5-fold cross-validation
cv_scores = cross_val_score(rf_model, X_train_scaled, y_train, cv=5, scoring='accuracy')

# Print the cross-validation scores
print(f"Cross-Validation Accuracy Scores: {cv_scores}")
print(f"Mean Cross-Validation Accuracy: {cv_scores.mean():.4f}")

"""## Interpretation:
The cross-validation accuracy scores show consistent performance across the different validation folds, with values ranging between 0.7860 and 0.8012. This suggests that your model's performance is relatively stable and does not vary too much depending on the subset of data used for training and testing.

A mean cross-validation accuracy of 0.7919 indicates that your model is correctly predicting the outcome about 79% of the time on unseen data, which is a decent performance.

## Plot Learning Curves:
You can plot learning curves to visualize overfitting by comparing training and validation performance over different numbers of training examples.
"""

from sklearn.model_selection import learning_curve

# Generate learning curves
train_sizes, train_scores, test_scores = learning_curve(
    rf_model, X_train_scaled, y_train, cv=5, scoring='accuracy', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10)
)

# Calculate mean and standard deviation for training and validation scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot learning curves
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training score')
plt.plot(train_sizes, test_mean, 'o-', color='red', label='Cross-validation score')

# Plot the std deviation as a shaded area
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='red')

plt.title('Learning Curves for Random Forest')
plt.xlabel('Training Size')
plt.ylabel('Accuracy Score')
plt.legend(loc='best')
plt.grid(True)
plt.savefig(os.path.join(output_path, 'Learning Curves.png'))  # Save the combined ROC plot
plt.show()

"""## Interpretation of the Learning Curve:
Training Accuracy:

The blue line, representing the training score, is consistently at 1.00 (100% accuracy), which indicates that the model is perfectly fitting the training data.
This is a sign that the model is likely overfitting because it is performing perfectly on the training data but may not generalize well to unseen data.


Cross-Validation Accuracy:
The red line, representing the cross-validation score, hovers around 0.79 (79% accuracy), which is significantly lower than the training score.
The gap between the training score (1.00) and the cross-validation score (0.79) shows that the model's performance drops significantly when applied to unseen data, another strong indicator of overfitting.


Shaded Region (Variance):
The shaded region around the cross-validation curve represents the variance in the cross-validation score across different folds.
Since the variance is small, it suggests that the model's performance is consistent across different cross-validation folds, but the generalization gap still exists.

Conclusion:
Overfitting is Present: The model is overfitting the training data, as it achieves perfect training accuracy but lower cross-validation accuracy. This indicates that the model is not generalizing well to unseen data.

## Tuning Random Forest:
"""

# Tuning the Random Forest model to reduce overfitting
rf_model_tuned = RandomForestClassifier(
    random_state=42,
    max_depth=10,           # Limit the depth of the trees
    min_samples_split=10,   # Increase the minimum number of samples to split
    min_samples_leaf=5,     # Increase the minimum number of samples per leaf
    n_estimators=100        # Number of trees in the forest
)

# Fit the tuned model
rf_model_tuned.fit(X_train_scaled, y_train)

# Evaluate the tuned model
train_accuracy_tuned = rf_model_tuned.score(X_train_scaled, y_train)
test_accuracy_tuned = rf_model_tuned.score(X_test_scaled, y_test)

print(f"Tuned Training Accuracy: {train_accuracy_tuned:.4f}")
print(f"Tuned Test Accuracy: {test_accuracy_tuned:.4f}")

"""tuned training accuracy of 0.7373 (73.73%) and the tuned test accuracy of 0.7283 (72.83%) indicate that the model is now less likely to be overfitting. The gap between the training and test accuracy has become much smaller compared to before, where the training accuracy was 1.00 and test accuracy was around 0.79.

## Tune Hyperparameters Further  using Random SearchCV
"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

# Define the parameter distribution
param_dist = {
    'n_estimators': randint(100, 300),
    'max_depth': randint(10, 30),
    'min_samples_split': randint(5, 15),
    'min_samples_leaf': randint(1, 5)
}

# Perform randomized search with 20 iterations
random_search = RandomizedSearchCV(RandomForestClassifier(random_state=42),
                                   param_distributions=param_dist,
                                   n_iter=20,  # Number of random combinations to try
                                   cv=5,
                                   scoring='accuracy',
                                   random_state=42)

random_search.fit(X_train_scaled, y_train)

# Best parameters from the random search
print(f"Best parameters: {random_search.best_params_}")

# Evaluate the model with the best parameters
best_rf_model = random_search.best_estimator_
test_accuracy_best = best_rf_model.score(X_test_scaled, y_test)
print(f"Test Accuracy with Best Parameters: {test_accuracy_best:.4f}")

"""Based on the results of RandomizedSearchCV, the best parameters for your Random Forest model are:

max_depth: 21
min_samples_leaf: 1
min_samples_split: 5
n_estimators: 158
Test Accuracy:
The Test Accuracy with these parameters is 0.7719 (77.19%), which is an improvement over the earlier tuned model accuracy of 72.83%.

## Feature Importance:
"""

# Get feature importance from the tuned model
importances = best_rf_model.feature_importances_
feature_names = X.columns

# Create a DataFrame for better readability
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Display and plot the top 10 important features
print("Top 10 Important Features:")
print(feature_importance_df.head(10))

# Plot
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(10))
plt.title('Top 10 Important Features in Random Forest')
plt.show()

"""## Confusion Matrix of Tuned Model"""

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Predict with the tuned model
y_pred_tuned = best_rf_model.predict(X_test_scaled)

# Plot and save the confusion matrix
plt.figure(figsize=(6, 4))
sns.heatmap(confusion_matrix(y_test, y_pred_tuned), annot=True, fmt='d', cmap='Blues')
plt.title('Tuned Random Forest Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.savefig(os.path.join(output_path, 'tuned_random_forest_confusion_matrix.png'))
plt.show()

"""Cross Validation"""

from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(best_rf_model, X_train_scaled, y_train, cv=5, scoring='accuracy')
print(f"Cross-Validation Accuracy Scores: {cv_scores}")
print(f"Mean Cross-Validation Accuracy: {cv_scores.mean():.4f}")

"""## Cross-Validation Summary:
Mean Accuracy: 76.17%, indicating consistent performance across different data subsets.

Test Accuracy: 77.19%, slightly higher than cross-validation, which is within an acceptable range.

Consistency: Cross-validation scores (0.7515 to 0.7698) show low variance, suggesting stable model performance.
"""





from sklearn.metrics import classification_report, roc_auc_score

# Evaluate on test set
y_pred_tuned = best_rf_model.predict(X_test_scaled)
print(classification_report(y_test, y_pred_tuned))

# ROC-AUC Score
y_pred_proba_tuned = best_rf_model.predict_proba(X_test_scaled)[:, 1]
auc_score = roc_auc_score(y_test, y_pred_proba_tuned)
print(f"ROC-AUC Score: {auc_score:.4f}")

"""## Classification Report Summary:
Accuracy (0.77): The model performs reasonably well, aligning with test and cross-validation results.
    
Non-Churn Class: High precision (0.77), recall (0.98), and F1-score (0.86), indicating strong performance for predicting non-churn customers.
    
Churn Class: Precision is good (0.82), but recall is low (0.21), meaning the model struggles to correctly identify churned customers (many false negatives).

Class Imbalance: The low recall for churn likely results from the class imbalance (more non-churn customers than churn customers).

ROC-AUC (0.8481): The model has good discriminatory power overall but is biased toward the non-churn class.

## Further Improvement :
Class Imbalance: Use class weights or SMOTE to improve recall for the churn class.

Threshold Tuning: Lower the decision threshold (e.g., to 0.4) to capture more churned customers.

Hyperparameter Tuning: Further tune parameters with RandomizedSearchCV or GridSearchCV.

Other Models: Try Gradient Boosting or XGBoost or neural network for potentially better performance.
"""

